# -*- coding: utf-8 -*-
"""first-order-model-demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/AliaksandrSiarohin/first-order-model/blob/master/demo.ipynb

# Demo for paper "First Order Motion Model for Image Animation"

**Clone repository**
"""

# !git clone https://github.com/AliaksandrSiarohin/first-order-model
# !pip install PyYAML==5.3.1
#
# cd first-order-model

"""**Mount your Google drive folder on Colab**"""

# from google.colab import drive
# drive.mount('/content/gdrive')

"""**Add folder https://drive.google.com/drive/folders/1kZ1gCnpfU0BnpdU47pLM_TQ6RypDDqgw?usp=sharing  to your google drive.**

**Load driving video and source image**
"""

import imageio
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from skimage.transform import resize
import os
# from IPython.display import HTML
import warnings
warnings.filterwarnings("ignore")

import threading
from tkinter import *
from tkinter.ttk import *
from tkinter.filedialog import askopenfile

root = Tk()
root.title("Morphizer")
root.geometry('400x200')
root.resizable(False, False)

image = None
video = None
def open_img(btn_text):
    global image
    image = askopenfile(mode ='r', filetypes =[('All Files', '*')])
    if image is not None:
        btn_text.set(image.name.split("/")[-1])
        # arint_func1(image) # <- Image function

def open_vid(btn_text):
    global video
    video = askopenfile(mode ='r', filetypes =[('All Files', '*')])
    if video is not None:
        btn_text.set(video.name.split("/")[-1])

        # arint_func2(video.name) # <- Video function

frame1 = Frame(root)
frame1.pack(side=TOP, padx = 25, pady = 25)

frame2 = Frame(root)
frame2.pack(side=TOP)

frame3 = Frame(root)
frame3.pack(side=TOP)

btn1_text = StringVar()
btn1_text.set('Choose Image')
btn1 = Button(frame1, textvariable=btn1_text, command = lambda:open_img(btn1_text))
btn1.pack(side = LEFT, padx = 10)

btn2_text = StringVar()
btn2_text.set('Choose Video')
btn2 = Button(frame1, textvariable=btn2_text, command = lambda:open_vid(btn2_text))
btn2.pack(side = LEFT, padx = 10)

btn3 = Button(frame1, text="GO", command = lambda:run2(image,video,textBox.get()))
btn3.pack(side = LEFT, padx = 10)

label = Label(frame2, text="Output file name: ")
label.pack(side=LEFT)

textBox = Entry(frame2, width=20)
textBox.pack(side=LEFT)

status_text = StringVar()
status = Label(frame3, textvariable=status_text, wraplength=375, justify=LEFT)
status.pack(side=TOP)

def run2(image, video, text):
    if __name__ == "__main__":
        t = threading.Thread(target=run, args=[image,video,text])
        t.start()


def run(im,vid,out):
    global status_text
    print(im.name,vid.name,out)
    if im is None or vid is None or out=="":
        print("Please complete required fields.")
        return
    # source_image = imageio.imread('C:/Users/Artin/first-order-model/MotionModels/armanFace3.png')
    # driving_video = imageio.mimread('C:/Users/Artin/first-order-model/MotionModels/Dam.mp4')
    status_text.set("Working")
    #update()
    try:
        source_image = imageio.imread(str(im.name))
        driving_video = imageio.mimread(str(vid.name))


        #Resize image and video to 256x256

        source_image = resize(source_image, (256, 256))[..., :3]
        driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]

        # def display(source, driving, generated=None):
        #     fig = plt.figure(figsize=(8 + 4 * (generated is not None), 6))
        #
        #     ims = []
        #     for i in range(len(driving)):
        #         cols = [source]
        #         cols.append(driving[i])
        #         if generated is not None:
        #             cols.append(generated[i])
        #         im = plt.imshow(np.concatenate(cols, axis=1), animated=True)
        #         plt.axis('off')
        #         ims.append([im])
        #
        #     ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)
        #     plt.close()
        #     return ani


        # HTML(display(source_image, driving_video).to_html5_video())

        """**Create a model and load checkpoints**"""

        from demo import load_checkpoints
        generator, kp_detector = load_checkpoints(config_path='config/vox-256.yaml',
                                    checkpoint_path='C:/Users/Artin/first-order-model/MotionModels/vox-cpk.pth.tar')

        """**Perform image animation**"""

        from demo import make_animation
        from skimage import img_as_ubyte

        predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=True)

        #save resulting video
        imageio.mimsave(out+'.mp4', [img_as_ubyte(frame) for frame in predictions])
        os.system("")
        status_text.set(f"File saved to {os.getcwd()}{out}.mp4")
    except Exception as e:
        status_text.set(f"{type(e)}: {e}")
#video can be downloaded from /content folder

# HTML(display(source_image, driving_video, predictions).to_html5_video())

"""**In the cell above we use relative keypoint displacement to animate the objects. We can use absolute coordinates instead,  but in this way all the object proporions will be inherited from the driving video. For example Putin haircut will be extended to match Trump haircut.**"""

# predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=False, adapt_movement_scale=True)
# HTML(display(source_image, driving_video, predictions).to_html5_video())
#
# """## Running on your data
#
# **First we need to crop a face from both source image and video, while simple graphic editor like paint can be used for cropping from image. Cropping from video is more complicated. You can use ffpmeg for this.**
# """
#
# #!ffmpeg -i /content/gdrive/My\ Drive/first-order-motion-model/07.mkv -ss 00:08:57.50 -t 00:00:08 -filter:v "crop=600:600:760:50" -async 1 hinton.mp4
#
# """**Another posibility is to use some screen recording tool, or if you need to crop many images at ones use face detector(https://github.com/1adrianb/face-alignment) , see https://github.com/AliaksandrSiarohin/video-preprocessing for preprcessing of VoxCeleb.**"""
#
# source_image = imageio.imread('/content/gdrive/My Drive/first-order-motion-model/09.png')
# driving_video = imageio.mimread('hinton.mp4', memtest=False)
#
#
# #Resize image and video to 256x256
#
# source_image = resize(source_image, (256, 256))[..., :3]
# driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]
#
# predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=True,
#                              adapt_movement_scale=True)
#
# HTML(display(source_image, driving_video, predictions).to_html5_video())


mainloop()
